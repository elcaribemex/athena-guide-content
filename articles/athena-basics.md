---
title: Athena basics
date: 2020-08-21
author: Theo Tolv
---
# Basic concepts in Athena

Athena is not an RDBMS or a data warehouse appliance, instead it's a query engine. Unlike most databases Athena doesn't manage the storage of your data, it just provides an interface for querying data stored on S3. In this way it is more like Spark than it is like Redshift. While most of your experience with SQL and databases is transferrable to Athena, there are a few things that are distinctly different and that you need to keep in mind to get the most out of it.

* [The origins and components of Athena](#the-origins-and-components-of-athena)
* [What problems Athena solves](#what-problems-athena-solves)
* [Key concepts](#key-concepts)
* [Running queries](#running-queries)
* [Permissions](#permissions)
* [Pricing Model](#pricing-model)

## The origins and components of Athena

You've probably heard that Athena is built on the open source query engine [Presto](https://prestosql.io). Unlike AWS' other similar services like their managed ElasticSearch, Kafka, and Cassandra offerings, Athena is not just a managed Presto cluster. In fact, AWS has another service that's is closer to managed Presto: Elastic MapReduce (EMR), which can be configured to run Presto clusters.

The fact that Athena is based on Presto shines through here and there, and AWS makes no secret of it – but for the most part Athena is Athena more than it is Presto. A big part of that is that it's an AWS service much more than hosted software. Instead of running a Presto cluster for you, it's a completely serverless setup where you just submit your queries, without worrying about anything else. It's also much more integrated into the AWS ecosystem than Presto is out of the box, but of course, also more limited – one USP of Presto is that it can connect to a lot of different backends, for example joining tables in MySQL with data in Google Sheets, while Athena only works with data on S3. _There is currently a preview of a feature called [Athena Federated Query](https://docs.aws.amazon.com/athena/latest/ug/connect-to-a-data-source.html) that makes it possible to query other backendds, but requires a lot of work on your part and is completely different from Presto's connectors_.

Athena is based on a fairly old version of Presto – 0.172 at the time of writing this, while the latest version is 340 (they dropped the "0." part between 0.215 and 300). However, the Athena team backports fixes, and passes their own fixes upstream, and the version number is probably useful mostly for looking up the documentation for Presto SQL syntax and functions at this point.

Athena was initially launched with its own data catalog, the component that keeps track of your databases and tables, but when Glue was launched it migrated to using Glue Data Catalog. Using Athena is really using three distinct AWS services; Athena itself, Glue, and S3 – and also IAM for permissions (there is also Lake Formation, but that's a topic for another post). This is something that you are aware of at almost all times, and something that trips up many new users, especially when it comes to permissions. There's not that many other AWS services where you need to know a lot about other services to use it.

## What problems Athena solves

I like to say that Athena activates all the data you already have on S3. Most companies that have used AWS for a while have probably accumulated a lot of random data on S3, logs of different kinds, backups, user data, etc. Perhaps you call this your "data lake", like so many startups do to inflate their valuation and enterprises do to seem to be on top of things. Whatever your pet name, before Athena you would either run jobs in EMR or on some home grown ETL "platform" to crunch the data into some useful format, which you stored in Redshift, some other RDBMS, or as new files on S3. There's a ton of tools in EMR for this, some of them are even good – but running clusters 24/7 is expensive, while running on demand is finicky, especially if you want to maintain state between runs.

Athena is an always-on query interface to all that data on S3, it makes it available and accessible, without hassle and with a fairly simple pricing model. Athena gets you quite a long way towards that dream of a data lake where the data you already have can be queried and combined without a lot of preprocessing or complex ETL.

Athena makes the data you have accessible and useful without forcing you to spend a lot of time making it accessible and useful.

While Athena is primarily a query engine, it has grown a few ETL features since its launch. You can [create new tables from query results](https://docs.aws.amazon.com/athena/latest/ug/ctas.html), which means that you don't need to use another service like EMR or Glue ETL for anything but the more complex ETL tasks. If all you want to do is some cleaning, transformation, and data format conversion, Athena and a little bit of management code will take you a very long way.

When you run a query in most RDBMS' you need to stick around until the response is ready, because their driver protocols are stateful. If you disconnect your query might get cancelled, and at least you'll have no way to retrieve the results. In contrast, Athena's API is asynchronous; when you start a query you get a query ID, which you can use at any point in time (up to a couple of months later) to check the query status. This means that your code doesn't need to run while Athena is processing a query – you can start a query as a reaction to some event, put the ID somewhere and at a much later point retrieve the results. This fits really well into the event driven serverless model where your code, for example a Lambda function, only runs while it's actually doing useful work._Since writing this, the [Redshift Data API](https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html) has been launched, which provides a very Athena-like API for interacting with Redshift._

I'll talk more about the pricing model below, but it's worth mentioning here too: Athena is priced very differently from services like Redshift, EMR, and Glue ETL. Instead of capacity/duration-based pricing Athena charges by the amount of data you process. For many use cases this is the problem that Athena solves: the cost of _not_ using Athena is zero. You don't have to think about how your Redshift cluster is burning a hole in your budget on weekends when no one is using it, for example.

## Key concepts

Athena does not work like the databases you might be used to. You don't load data into Athena, you create tables that describe the data you already have. The approach taken by Athena is called "schema-on-read", which means that it's only when you run a query that table schemas are taken into consideration. You can create any number of tables with whatever columns and configuration you want, it's only when you try to use them that Athena will complain if they don't match the reality of your data. At the same time, as long as some data is found in a table's location, and it kind of looks like what you've said it would look like, Athena will do something with it.

### The catalog: databases, tables, and views

Athena uses Glue Data Catalog to look up things like databases and tables. A "catalog" in the terminology used by Athena means a place where the metadata about your data is stored, things like tables that describe the data and where it can be found.

You can create catalog entries through DDL statements (i.e. SQL) through Athena, or using the Glue Data Catalog API directly. In theory, tables created in the Glue Data Catalog from Spark on EMR or Glue ETL will also be available to Athena, although the stars need to align properly for this to actually work in practice. When you run DDL statements through Athena, it translates your request into Glue Data Catalog API calls behind the scenes.

Glue Data Catalog has two types of catalog entries: databases and tables. Databases are collections of tables, and are mostly for organizing tables into manageable namespaces. Tables describe the location of data, as well as how it's stored (e.g. file format, compression, etc.), how it's organized (partitioned or not, more on that below), the properties found on data items (i.e. columns) and their types, as well as other metadata that is used to describe and interpret the data. It's mostly up to the clients of the Glue Data Catalog how to encode this information, which is the reason that interoperability is mostly theoretical – but there is enough flexibility in this model for tools like Glue Crawler to add its metadata without interfering with Athena, for example.

Athena supports a third kind of catalog entry: views. These are encoded as tables in the Glue Data Catalog, with some extra metadata that make them appear as views to Athena, but as gibberish to other clients looking in the catalog.

Glue Data Catalog doesn't validate anything you put into it, except for basic things like required fields and the length of names, and neither does Athena. Creating a database or a table doesn't do anything to the data that it describes – in fact, it doesn't even need to describe data that actually exists. You can create tables for completely imaginary data sets, or data sets you have not yet created. Catalog entries are light-weight entities that you can throw about however you want. An important consequence of this to be aware of is that unlike most other databases, dropping a table does not affect any data – you can iterate on a table schema by creating and dropping tables without fear of changing or loosing your data.

Being able to create a table for data that already exist can be mind blowing if you've not used something like Athena before, but being able to create a table for data that doesn't yet exist might not sound like much of a feature – that's how most databases work. However, I've noticed that people sometimes think it's one or the other, and that's not the case. It's really useful to be able to set up tables as part of the infrastructure – I often do it with CloudFormation and Terraform – so that once data starts coming in it's available to be queried. If I notice I've made a mistake in the schema I update the template and the stack until I've gotten it into shape.

Never worry about recreating tables if you find that you've made a mistake. Using the Glue Data Catalog API you can even update a table without any downtime at all (although be careful if the table has partitions, you might make it unusable if the partitions no longer match the table). You can iterate on a table schema until you find something that works with the data you have, instead of transforming the data to fit into a table. Athena handles a lot of different data formats and ways of organizing data, and you should almost never have to transform your data to work with Athena.

Also don't worry about having multiple tables pointing to the same data, or parts of a data set that another table points to.

### Partitions and partitioning

You might have read that Athena doesn't have any indexes. In RDBMS' indexes makes queries fast by making it possible to find the right data, and skip all the data that is not relevant to a query. Since it doesn't have indexes, does this mean that Athena always reads all data in a table? Yes and no.

If you create a regular table for data in a format such as CSV or JSON the answer is yes, Athena will read every single byte in the table's location for every query (unless there's a `LIMIT` clause in which case it will attempt to read only as much as it needs to return the desired number of rows, but that's a special case). If the data is in in a columnar format, e.g. Parquet or ORC, Athena can make use of metadata within the files to read only the columns referenced by the query, and skip entire blocks when possible (although in my experience this doesn't work as well with ORC files, so I stick with Parquet when I can), but even if it can skip parts of files it will read every single file.

Reading all the data for a table for every query can get expensive, and slow down queries, especially if the data set grows over time. Even using Parquet will not work well enough in the long run, so what to do?

The answer, as you might have guessed from the title of this section, is partitioning. Partitioning a data set means organizing its data by one or more properties so that queries can quickly skip parts of the data set that doesn't match. A data set about sales could be partitioned by customer so that queries looking at only one or a few customers can ignore most of the data set. In practice the way you partition is simply to put data into a directory structure (prefixes for the S3 purists) where the directories are named from the property values. A very common partitioning scheme is to put data into directories named from the date so that a query looking at the most recent data quickly can skip all old data.

When you set up a table in Athena you include information about how the data is partitioned by specifying one or more partition keys, these correspond to the directory hierarchy of the data set. If the data is organized by customer and date, for example `data/acme/2020-08-01/data.json`, the partition keys could be `customer` and `date`, e.g. `PARTITIONED BY (customer string, date string)`. When querying you can say `WHERE customer = 'acme'` and Athena will know it only has to look in the `data/acme/` directory and can skip everything else – and if you also include a filter on the data it can narrow down the list of files to process even further.

When you create a regular table you can run queries against it straight away. Athena will look at the table's location field and processes all the files it finds. Partitioned tables also have a location, but that's just because it is required by the Glue Data Catalog. To be able to query a partitioned table you need to tell Athena about the partitions of the data set. This can be done in many different ways, and I cover them in more detail in [Five ways to add partitions](/articles/five-ways-to-add-partitions). The short version is that you either create partitions in the Glue Data Catalog (through Athena DDL statements or API calls), or you configure the table with metadata that makes it possible for Athena to figure out the location of partitions (this is called [Partition Projection](https://docs.aws.amazon.com/athena/latest/ug/partition-projection.html)).

Almost all data sets are already partitioned in some way, it's very rare in my experience to find a non-trivial data set where all files are just in a single directory without any organization to it. The most common form of partitioning is based on time. Data sets tend to grow over time, and adding new data to directories named from the date makes a lot of sense. In general, data sets are often organized in a way that makes it easy for the producing side, for example to make it possible to replace a piece of data if it changes.

[AWS CloudTrail is a good example of how data sets tend to get organized](https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html). CloudTrail delivers logs to S3 like this: `s3://some-bucket/AWSLogs/0123456789/CloudTrail/us-east-1/2020/08/21/0123456789_CloudTrail_us-east-1_20200821T2120Z_W8LDOrF9dU4NZ7Kt.json.gz`. This URI contains (after `CloudTrail` which is the root), the region, the year, month, and day, and the file name. This partitioning scheme makes it easy to find all files pertaining to a specific AWS region, and if you only want to query the last month, day, or hour's worth of logs a query can easily skip most of the data set. This data set can be said to be partitioned by region, year, month, and day (or by region and date, you don't have to have a one-to-one mapping between partition keys and path components).

A lot of what is written about partitioning in Athena uses a partitioning style that looks like `data/region=us-east-1/year=2020/month=08/day=21/data.json`. This is called [Hive style partitioning](/articles/five-ways-to-add-partitions) and is common in the Hadoop ecosystem. It has some benefits, but even though the Athena documentation may make it look like this is _the_ way to partition data you don't have to use this scheme, and it's extremely uncommon to see outside of the Hadoop world.

You can map the partitioning of most data sets to Athena tables, but there is one situation that Athena does not handle: files with different schemas in the same directory. Someone might have decided to export data into directories named from the date, but mix files representing different kinds of data in these. For example `data/2020-08-21/orders.csv` and `data/2020-08-21/invoices.csv`. This might make sense when producing the data, but is unfortunately incompatible with the way Athena works. Athena will process all files found in a table or partition's location, and there is no way to configure it to filter based on file name.

## Running queries

Unlike most RDBMS', Athena has an asynchronous API. When you submit a query you get a "query execution ID" back, and the API call completes immediately. To track the progress of a query you use the ID in another API call to poll its status, and yet another API cal to retrieve results when the query has completed. The asynchronous query execution API is one of my favorite features of Athena, as it enables usage patterns that aren't possible with a synchronous model. For the situations where a synchronous connection-based model is needed, for example tools that expect a JDBC or ODBC driver, this can be built on top of the asynchronous API, and there are also offical and unofficial drivers available, more on this later.

Running a query in Athena involves three API calls: [`StartQueryExecution`](https://docs.aws.amazon.com/athena/latest/APIReference/API_StartQueryExecution.html), [`GetQueryExecution`](https://docs.aws.amazon.com/athena/latest/APIReference/API_GetQueryExecution.html), and [`GetQueryResults`](https://docs.aws.amazon.com/athena/latest/APIReference/API_GetQueryResults.html). To run a query you use `StartQueryExecution` and pass the SQL you want to run, as well as an S3 location where Athena can write the results – since queries are run asynchronously Athena needs somewhere to store the results so that you can ask for them later. The `StartQueryExecution` returns a "query execution ID", which you use with the other two API calls to identify which query you want status or results for.

It's important to understand that when you call `StartQueryExecution` you really only submit the query to Athena, and just because you don't get an error back does not mean query has succeeded, or will succeed. It performs some validation, for example checking that the syntax of the SQL is valid, but semantic SQL errors, like referring to functions, tables, or columns that don't exist, will not be caught here. Therefore it's important that you always use `GetQueryExecution` to check that the query didn't fail early because of a typo in a function or table name.

While the query is running you use `GetQueryExecution` to check its status. In most cases you need to make this API call multiple times until the query completes, and there is no API to wait, or block, until the query completes. Often what you do is that you set up a loop that makes this API with a small delay between each call. I like to implement a variant of exponential backoff so that I wait longer between calls the longer the query has been running, up to a max interval, but you can make as many calls as you want, AWS does not charge for these calls.

The result of the `GetQueryExecution` API call contains a lot of information, and for some reason the designers decided to bury the status fairly deep inside a nested structure. When submitted the status will be either `QUEUED` or `RUNNING`, the former meaning that it's waiting for resources, and the latter that resources have been allocated and the query has started executing. If the query fails it gets status `FAILED` and an error message will be available in the response from the API call. Queries can also be cancelled with the [`StopQueryExecution`](https://docs.aws.amazon.com/athena/latest/APIReference/API_StopQueryExecution.html) API call, in which case they end up in status `CANCELLED`. Hopefully your query instead ends up with status `SUCCEEDED`, in which case you can call `GetQueryResults` to retrieve the results.

`GetQueryResults` returns the result rows, as well as column metadata. All values are returned as strings, but the metadata contains type information that can be used to format the values correctly. The results are paged, with a default page size of 1,000 rows, which is also the maximum page size. Make sure you look for the token that indicates whether or not there are subsequent pages so that you don't miss any part of the result set.

Running `GetQueryResults` is only possible after a query has status `SUCCEEDED`, calling this API while a query is running, or with the ID of a failed query results in an error.

To reiterate: running a query means calling `StartQueryExecution`, and then use the ID it returns to run `GetQueryExecution` over and over again until the status is `FAILED`, `CANCELLED`, or `SUCCEEDED`. In case of the latter you finally run `GetQueryResults`, until there are no more result pages, in order to retrieve the results.

### Raw results

There is an alternative to using the `GetQueryResults` API call: as I mentioned above you must supply Athena with a location where it can write the query results, and the `GetQueryExecution` API call also contains the location of the file that Athena has written. This file is just a plain CSV that can be retrieved and processed with any tool that supports CSV. Along with the CSV there will also be a file with a `.metadata` suffix. This is a [ProtoBuf](https://developers.google.com/protocol-buffers/) encoded file containing the result metadata, including column types. There is no offical documentation for this file, but I have made some [investigations into how to interpret these files](https://gist.github.com/iconara/4969c247e8abb69600cdbe6f4b20f50b).

Reading the CSV directly from S3 instead of using the `GetQueryResults` API can in many situations give you better performance, and it can also be useful when you want to use the query results in another tool that can read CSV, such as Excel.

Using CSV as the result format for Athena was in my opinion not the best choice. CSV is the lingua franca of data science in a way, but it's also an extremely messy file format. The particular variant Athena uses means that it will [render array and map output ambiguously](https://athena.guide/articles/complex-types#complex-types-in-results), and not casting these types to JSON can result in results that are not possible to parse correctly.

When running DDL the results are not written as CSV, but plain text, or in some cases a binary format which I have not been able to decode.

### Work groups

Athena runs queries in the context of a "work group". By default you get a work group called "primary", which you can use for everything if you want to – it's completely optional to use the features provided by the work groups.

Work groups solve a mix of different problems that existed in Athena before they were introduced. Just as Athena is one big cluster shared by all AWS customers in the same region, all applications in the same account used the same Athena service, and there was no way to determine how much of the total Athena charge on the bill that was caused by any one particular application or data scientist. There was also no way to set quotas or permissions on an application basis, or even know how much each application was using Athena.

If you create a work group per application you can set quotas, get metrics, and costs reported on a per-application basis.

The quota is unfortunately only on the total bytes scanned, and is primarily a feature designed as a circuit breaker to avoid runaway charges. Your account still has one global concurrency quota which can't be divided up between work groups.

Another feature of work groups is that they can have defaults for some of the parameters you set when you run a query. The output location can be set (and also enforced) in the work group, which can be a convenient way to avoid applications having to have know about where results are stored.

### JDBC/ODBC

Many tools expect to interact with a database using JDBC and ODBC and not a custom API. For that reason Athena have commissioned [JDBC and OBDC drivers](https://docs.aws.amazon.com/athena/latest/ug/athena-bi-tools-jdbc-odbc.html). Internally these use the same API described above (although using a currently private API for retrieving the results). The drivers are not written by the Athena team, but by a company called Simba, who have written [drivers for a lot of other RDBMS'](https://www.simba.com/drivers/).

In my experience the offical JDBC driver leaves a lot to desire, and has been plagued with bugs that don't get fixed for years. For that reason I wrote an [alternative JDBC driver](http://github.com/burtcorp/athena-jdbc/) when I was at a company that required much higher performance and quality than the offical driver could deliver. If you need a JDBC driver for your project I recommend you start with the official driver, it's going to continue to be developed and support new features as they are released, but do have a look at the alternative driver if you are interested or have had problems with the official.

### Concurrency limits & noisy neighbors

Athena runs all queries in a shared cluster. While the service goes to great lengths to ensure that your queries and data is isolated from other customers and secure, you all share the same pool of compute resources. Each account gets a quota that determines how many concurrent queries it can run, and exceeding this limit results in throttling errors when submitting queries. The default limit is 20 concurrent queries (DDL statements have the same limit, but a separate quota), and you can ask AWS for this to be raised if you have a legitimate need.

Even if you stay within your quota there is however no guarantee that your queries will run immediately. You share the cluster with all other customers and that means that sometimes there are no compute resources available when you submit a query. In this scenario your query will be queued until resources become available. Athena is used for a lot of reporting applications and these tend to be configured to run jobs at specific times of the day, almost always the top of the hour. If you run a lot of queries you can notice how the amount of time your queries spend in the queue spikes around the top of every hour. This can significantly affect performance and is something that you should be aware of, especially if you are trying to optimize your queries – always look at the statistics returned by the `GetQueryExecution` API call to check how much time your query spent in the queue before making assumptions about the performance of your query or data set.

## Permissions

Permissions in Athena are managed through IAM, unless you use Lake Formation (which is a topic in itself and not covered here). As I've mentioned above, Athena is not an isolated service, and running a query involves at least three AWS services: Athena, Glue Data Catalog, and S3. This is reflected in the permissions model too: to run a query Athena will use the Glue Data Catalog on your behalf, as well as list and read files on S3, and you will need permissions for all of this in order for the query to succeed. This is unlike invoking a Lambda function where the function has its own set of permissions that govern the actions of the function. Athena instead proxies your permissions when it performs actions on other services (again, catalogs manged by Lake Formation have a different model, more similar to that of Lambda).

Because there are multiple services involved, IAM policies for Athena often have a lot of statements, and they can be hard to get right in the beginning. Each service has its resources and ways of specifying and limiting permissions.

Athena is probably the simplest of them, you really only need to make sure the principal (i.e. user or role) has permission to the API calls involved in running a query, which means the actions `athena:StartQueryExecution`, `athena:GetQueryExecution`, and `athena:GetQueryResults` for the [workgroup](https://docs.aws.amazon.com/athena/latest/ug/manage-queries-control-costs-with-workgroups.html) that the query runs in. Next is S3, where `s3:ListBucket` and `s3:GetObject` are needed for the bucket and objects that will be read, and `s3:PutObject` and `s3:GetObject` where the results will be written. Finally, Glue's IAM permissions are probably the hardest to get right, partly because it's hard to know which API calls Athena makes behind the scenes and therefore needs permissions for, and partly because Glue requires you to specify permissions on all levels of its catalog hierarchy – granting permission to a table is not enough, you also need to grant permission to the database the table is in, and the catalog the database is in. Luckily the [Athena documentation has example policies for the most common use cases](https://docs.aws.amazon.com/athena/latest/ug/fine-grained-access-to-glue-resources.html).

The permissions model is far from perfect, and it has a very steep learning curve, but I think there are benefits to it too. I like that it's transparent that Athena uses the other two services, and that it makes the API calls to them in the same way, with the same permissions, as if the principal had done it themselves – and that it shows up in CloudTrail in that way too.

A side effect of the permissions model is that a principal that is allowed to query a table will also be allowed to download all the files belonging to that table. In most cases this is not really an issue, the same data can after all be downloaded by making SQL queries, but there may be situations where the principal is only allowed to query views that aggregate the data or tables where some properties present in the data are not mapped to columns, or situations where you just don't want to provide access to the raw data. In these cases you can use the [`aws:calledVia`](https://aws.amazon.com/blogs/security/how-to-define-least-privileged-permissions-for-actions-called-by-aws-services/) condition on the S3 statements to say that they are only allowed to be performed by the Athena service, not by the principal directly.

## Pricing model

In the context of AWS, Athena's pricing model is refreshingly simple, at least on the surface. It's a flat $5 per TB of data scanned, rounded to the nearest MB, with a 10 MB minimum. It's the amount of data read from S3 that is measured, which means that querying compressed data is cheaper, and using columnar formats that allow Athena to skip columns and skip blocks also lower query costs significantly.

As will all AWS services, the devil is in the details. In addition to the Athena charges, you also pay for the Glue Data Catalog and S3 operations Athena performs. While I've never seen Glue become a big cost, I've more than once seen uses of Athena where the number of S3 operations is the cost driver.

Glue has a very generous free tier where you can store a million objects (databases, tables, partitions), and perform a million operations per month for free, and only then pay $1 per hundred thousand objects stored, and $1 per million operations, meaning it will take a while until you start paying for Glue at all, and probably only for operations unless you have tables with hundreds of thousands of partitions.

S3's pricing page has so many zeroes in the charges that it's sometimes hard to think that you'll ever end up paying anything at all. However, list operations actually cost $5 per million, and get operations $0.4 per million (in most regions, there are a few where it's slightly more expensive). If you run a query against a table with a million files you will be charged for a million get operations, and at least a thousand list operations (exactly how many depends on how the data is organized, but with S3's max page size of 1,000 it can't be done in fewer operations than that). This will cost $0.4 for the get operations and $0.005 for the list operations. For comparison, if each file is 1 MB, making the whole data set 1 GB, the Athena charges would be $0.5.

While that scenario might not be very common, I have seen setups where Athena is used as the query engine for data sets with lots of small files and where the S3 costs outweigh the Athena costs by an order of magnitude.

When considering Athena you should take the number of files and how they are organized into account. If you can, try to limit the number of files, and keep directory hierarchies as flat as possible (Athena will traverse your data set as if it were a file system, so every directory will result in a list operation). How to optimize small data sets for costs is a separate topic I hope to be able to cover in the future.

At the other end of the spectrum, the pricing model is more predictable. If you have data sets that are hundreds of GB to multiples of TB or even larger, you probably don't have millions of small files, and it will be the cost per TB that dominates. In this scenario, finding optimal ways to partition the data set, and making use of columnar file formats will be key to managing costs. However, each query will have a real and significant cost to it, and mistakes and misconfiguration can rack up huge bills.

I really wish there were better tools available to predict the costs of queries. When you run a query you get the total bytes scanned, which means that you at least after the fact know the Athena charge for a particular query. Using CloudTrail you can also figure out what Glue and S3 operations were made. Using these two tools you can optimize the cost of queries. You can use the bytes scanned metric to make sure Athena takes advantage of your partitioning, and you can use CloudTrail to see how it lists and retrieves files from S3 to figure out if there are better ways you could organize your files to avoid S3 operations. It's a lot of hard work, but at least the data is there.
